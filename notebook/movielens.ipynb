{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poeem search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“tf”的单元格需要ipykernel包。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n tf ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import numpy as np\n",
    "import poeem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000263 entries, 0 to 20000262\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Dtype  \n",
      "---  ------     -----  \n",
      " 0   userId     int64  \n",
      " 1   movieId    int64  \n",
      " 2   rating     float64\n",
      " 3   timestamp  int64  \n",
      "dtypes: float64(1), int64(3)\n",
      "memory usage: 610.4 MB\n"
     ]
    }
   ],
   "source": [
    "data_rate = pd.read_csv('../data/ratings.csv')\n",
    "data_rate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:138493 min:1 118205    9254\n",
      "8405      7515\n",
      "82418     5646\n",
      "121535    5520\n",
      "125794    5491\n",
      "          ... \n",
      "89305       20\n",
      "110463      20\n",
      "96990       20\n",
      "134747      20\n",
      "6526        20\n",
      "Name: userId, Length: 138493, dtype: int64\n",
      "max:131262 min:1 296       67310\n",
      "356       66172\n",
      "318       63366\n",
      "593       63299\n",
      "480       59715\n",
      "          ...  \n",
      "125545        1\n",
      "78873         1\n",
      "112907        1\n",
      "112909        1\n",
      "110510        1\n",
      "Name: movieId, Length: 26744, dtype: int64\n",
      "max:5.0 min:0.5 4.0    5561926\n",
      "3.0    4291193\n",
      "5.0    2898660\n",
      "3.5    2200156\n",
      "4.5    1534824\n",
      "2.0    1430997\n",
      "2.5     883398\n",
      "1.0     680732\n",
      "1.5     279252\n",
      "0.5     239125\n",
      "Name: rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('max:{}'.format(data_rate['userId'].max()), 'min:{}'.format(data_rate['userId'].min()), data_rate['userId'].value_counts())\n",
    "print('max:{}'.format(data_rate['movieId'].max()), 'min:{}'.format(data_rate['movieId'].min()), data_rate['movieId'].value_counts())\n",
    "print('max:{}'.format(data_rate['rating'].max()), 'min:{}'.format(data_rate['rating'].min()), data_rate['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000263, 138493, 26744)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_all = len(data_rate)\n",
    "user_all = list(data_rate['userId'].unique())\n",
    "movie_all = list(data_rate['movieId'].unique())\n",
    "N_user, N_movie = len(user_all), len(movie_all)\n",
    "N_all, N_user, N_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138493, 26744)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_all = list(data_rate['userId'].unique())\n",
    "movie_all = list(data_rate['movieId'].unique())\n",
    "len(user_all), len(movie_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_stratified_into_train_val_test(df_input, stratify_colname='y',\n",
    "#                     frac_train=0.6, frac_val=0.2, frac_test=0.2,\n",
    "#                     random_state=None):\n",
    "#     '''\n",
    "#     Splits a Pandas dataframe into three subsets (train, val, and test)\n",
    "#     following fractional ratios provided by the user, where each subset is\n",
    "#     stratified by the values in a specific column (that is, each subset has\n",
    "#     the same relative frequency of the values in the column). It performs this\n",
    "#     splitting by running train_test_split() twice.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df_input : Pandas dataframe\n",
    "#         Input dataframe to be split.\n",
    "#     stratify_colname : str\n",
    "#         The name of the column that will be used for stratification. Usually\n",
    "#         this column would be for the label.\n",
    "#     frac_train : float\n",
    "#     frac_val   : float\n",
    "#     frac_test  : float\n",
    "#         The ratios with which the dataframe will be split into train, val, and\n",
    "#         test data. The values should be expressed as float fractions and should\n",
    "#         sum to 1.0.\n",
    "#     random_state : int, None, or RandomStateInstance\n",
    "#         Value to be passed to train_test_split().\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     df_train, df_val, df_test :\n",
    "#         Dataframes containing the three splits.\n",
    "#     '''\n",
    "\n",
    "#     if frac_train + frac_val + frac_test != 1.0:\n",
    "#         raise ValueError('fractions %f, %f, %f do not add up to 1.0' % \\\n",
    "#                          (frac_train, frac_val, frac_test))\n",
    "\n",
    "#     if stratify_colname not in df_input.columns:\n",
    "#         raise ValueError('%s is not a column in the dataframe' % (stratify_colname))\n",
    "\n",
    "#     X = df_input # Contains all columns.\n",
    "#     y = df_input[[stratify_colname]] # Dataframe of just the column on which to stratify.\n",
    "\n",
    "#     # Split original dataframe into train and temp dataframes.\n",
    "#     df_train, df_temp, y_train, y_temp = train_test_split(X,\n",
    "#                                 y,\n",
    "#                                 stratify=y,\n",
    "#                                 test_size=(1.0 - frac_train),\n",
    "#                                 random_state=random_state)\n",
    "\n",
    "#     # Split the temp dataframe into val and test dataframes.\n",
    "#     relative_frac_test = frac_test / (frac_val + frac_test)\n",
    "#     df_val, df_test, y_val, y_test = train_test_split(df_temp,\n",
    "#                                 y_temp,\n",
    "#                                 stratify=y_temp,\n",
    "#                                 test_size=relative_frac_test,\n",
    "#                                 random_state=random_state)\n",
    "\n",
    "#     assert len(df_input) == len(df_train) + len(df_val) + len(df_test)\n",
    "\n",
    "#     return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train, data_val, data_test = split_stratified_into_train_val_test(data_rate, 'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contains_all(df, user_all, movie_all):\n",
    "    user_list, item_list = [], []\n",
    "    user_unique, movie_unique = list(df['userId'].unique()), list(df['movieId'].unique())\n",
    "    # for user in  user_all:\n",
    "    #     if user not in user_unique:\n",
    "    #         user_list.append(user)\n",
    "    #         # print('userId:{}'.format(user))\n",
    "    #         # return False\n",
    "    # for movie in movie_all:\n",
    "    #     if movie not in movie_unique:\n",
    "    #         item_list.append(movie)\n",
    "    #         # print('movie:{}'.format(movie))\n",
    "    #         # return False\n",
    "    # # return True\n",
    "    # return user_list, item_list\n",
    "    return len(user_all)-len(user_unique), len(movie_all)-len(movie_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12000157, 4000053, 4000053, 20000263)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "while cnt<100:\n",
    "    data_train, data_val, data_test = np.split(data_rate.sample(frac=1),\n",
    "                            [int(.6*len(data_rate)), int(.8*len(data_rate))])\n",
    "    u, m = check_contains_all(data_train, user_all, movie_all)\n",
    "    if u+m == 0:\n",
    "        break\n",
    "    cnt += 1\n",
    "    print(cnt)\n",
    "len(data_train), len(data_val), len(data_test), len(data_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138493, 138493)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_rate['userId'].unique()), len(data_train['userId'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1957)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_left, movie_left = check_contains_all(data_train, user_all, movie_all)\n",
    "user_left, movie_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.to_csv('../data/ratings_train.csv')\n",
    "data_test.to_csv('../data/ratings_test.csv')\n",
    "data_val.to_csv('../data/ratings_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_user, N_movie = 138493, 26744"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poeem Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCH = 3\n",
    "EMB_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    query_column = tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key='query',\n",
    "            vocabulary_list=range(N_user),\n",
    "            dtype=tf.int32),\n",
    "        dimension=EMB_DIM)\n",
    "    item_column = tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key='item',\n",
    "            vocabulary_list=range(N_movie),\n",
    "            dtype=tf.int32),\n",
    "        dimension=EMB_DIM)\n",
    "    \n",
    "    query_emb = tf.feature_column.input_layer(features, [query_column])\n",
    "    item_emb = tf.feature_column.input_layer(features, [item_column])\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, predictions={'query': query_emb, 'item': item_emb})\n",
    "\n",
    "    def cosine(a, b):\n",
    "        a = tf.nn.l2_normalize(a, axis=1)\n",
    "        b = tf.nn.l2_normalize(b, axis=1)\n",
    "        return tf.matmul(a, b, transpose_b=True)\n",
    "\n",
    "    scores = cosine(query_emb, item_emb)\n",
    "\n",
    "    batch_size = tf.shape(query_emb)[0]\n",
    "    loss = tf.reduce_sum(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=tf.eye(batch_size),\n",
    "            logits=scores * 30))  # 1/30 is softmax temperature. Not carefully tune.\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(LEARNING_RATE)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, train_op=train_op, predictions={'query': query_emb, 'item': item_emb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices({'query': data_train['userId'].to_numpy().astype(np.int32), \n",
    "                            'item': data_train['movieId'].to_numpy().astype(np.int32)})\n",
    "    dataset = dataset.shuffle(buffer_size=1000).batch(BATCH_SIZE).repeat(EPOCH)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 18:09:29.732227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 0000:08:00.0\n",
      "2023-02-13 18:09:29.732704: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:09:29.732905: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:09:29.733093: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:09:29.733266: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:09:29.733436: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:09:29.733606: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:09:29.733811: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:09:29.733836: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-13 18:09:29.733873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-13 18:09:29.733891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2023-02-13 18:09:29.733903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()\n",
    "\n",
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False,\n",
    "    min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp5d8_bjg7\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp5d8_bjg7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faad1dcce10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-13 18:05:15.350125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
      "name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 0000:08:00.0\n",
      "2023-02-13 18:05:15.350599: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:05:15.350796: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:05:15.350974: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:05:15.351129: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:05:15.351285: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:05:15.351442: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:05:15.351600: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu\n",
      "2023-02-13 18:05:15.351620: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1641] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-02-13 18:05:15.351655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-02-13 18:05:15.351672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
      "2023-02-13 18:05:15.351683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp5d8_bjg7/model.ckpt.\n",
      "INFO:tensorflow:loss = 12673.435, step = 1\n",
      "INFO:tensorflow:global_step/sec: 74.1644\n",
      "INFO:tensorflow:loss = 12959.156, step = 101 (1.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3093\n",
      "INFO:tensorflow:loss = 12405.019, step = 201 (1.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.728\n",
      "INFO:tensorflow:loss = 11874.441, step = 301 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.6485\n",
      "INFO:tensorflow:loss = 11310.115, step = 401 (1.272 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.2063\n",
      "INFO:tensorflow:loss = 10969.152, step = 501 (1.386 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.4474\n",
      "INFO:tensorflow:loss = 10391.017, step = 601 (1.275 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.43\n",
      "INFO:tensorflow:loss = 9868.72, step = 701 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1236\n",
      "INFO:tensorflow:loss = 9381.947, step = 801 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.5481\n",
      "INFO:tensorflow:loss = 8987.032, step = 901 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 75.3815\n",
      "INFO:tensorflow:loss = 8656.922, step = 1001 (1.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 64.7194\n",
      "INFO:tensorflow:loss = 8326.803, step = 1101 (1.547 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.5157\n",
      "INFO:tensorflow:loss = 7917.7847, step = 1201 (1.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9388\n",
      "INFO:tensorflow:loss = 7998.7803, step = 1301 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2899\n",
      "INFO:tensorflow:loss = 7598.8076, step = 1401 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.352\n",
      "INFO:tensorflow:loss = 7502.999, step = 1501 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.122\n",
      "INFO:tensorflow:loss = 7406.4854, step = 1601 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4583\n",
      "INFO:tensorflow:loss = 7129.6523, step = 1701 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.2288\n",
      "INFO:tensorflow:loss = 7154.3975, step = 1801 (1.312 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7058\n",
      "INFO:tensorflow:loss = 7216.0444, step = 1901 (1.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7448\n",
      "INFO:tensorflow:loss = 7175.2075, step = 2001 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.897\n",
      "INFO:tensorflow:loss = 7125.013, step = 2101 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.7343\n",
      "INFO:tensorflow:loss = 7077.0137, step = 2201 (1.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6068\n",
      "INFO:tensorflow:loss = 7016.7983, step = 2301 (1.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9015\n",
      "INFO:tensorflow:loss = 6966.6826, step = 2401 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.2272\n",
      "INFO:tensorflow:loss = 6972.128, step = 2501 (1.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.744\n",
      "INFO:tensorflow:loss = 7001.2085, step = 2601 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2266\n",
      "INFO:tensorflow:loss = 6866.1426, step = 2701 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3705\n",
      "INFO:tensorflow:loss = 6914.879, step = 2801 (1.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9007\n",
      "INFO:tensorflow:loss = 6854.35, step = 2901 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3455\n",
      "INFO:tensorflow:loss = 6873.968, step = 3001 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4748\n",
      "INFO:tensorflow:loss = 6798.0283, step = 3101 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1447\n",
      "INFO:tensorflow:loss = 6732.9326, step = 3201 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 86.145\n",
      "INFO:tensorflow:loss = 6797.588, step = 3301 (1.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4898\n",
      "INFO:tensorflow:loss = 6686.4097, step = 3401 (1.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.3158\n",
      "INFO:tensorflow:loss = 6776.8594, step = 3501 (1.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6537\n",
      "INFO:tensorflow:loss = 6735.497, step = 3601 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.2019\n",
      "INFO:tensorflow:loss = 6713.3125, step = 3701 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8035\n",
      "INFO:tensorflow:loss = 6818.0493, step = 3801 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 76.9901\n",
      "INFO:tensorflow:loss = 6679.1216, step = 3901 (1.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.151\n",
      "INFO:tensorflow:loss = 6650.71, step = 4001 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1047\n",
      "INFO:tensorflow:loss = 6726.2314, step = 4101 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.8462\n",
      "INFO:tensorflow:loss = 6696.0312, step = 4201 (1.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.4644\n",
      "INFO:tensorflow:loss = 6687.5527, step = 4301 (1.199 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2684\n",
      "INFO:tensorflow:loss = 6642.089, step = 4401 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9727\n",
      "INFO:tensorflow:loss = 6587.784, step = 4501 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.327\n",
      "INFO:tensorflow:loss = 6616.0244, step = 4601 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8568\n",
      "INFO:tensorflow:loss = 6623.8164, step = 4701 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4257\n",
      "INFO:tensorflow:loss = 6642.6914, step = 4801 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6407\n",
      "INFO:tensorflow:loss = 6565.159, step = 4901 (1.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.3084\n",
      "INFO:tensorflow:loss = 6626.6987, step = 5001 (1.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.9276\n",
      "INFO:tensorflow:loss = 6553.4443, step = 5101 (1.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0456\n",
      "INFO:tensorflow:loss = 6548.152, step = 5201 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4077\n",
      "INFO:tensorflow:loss = 6564.325, step = 5301 (1.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.809\n",
      "INFO:tensorflow:loss = 6591.907, step = 5401 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.7205\n",
      "INFO:tensorflow:loss = 6649.974, step = 5501 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.3592\n",
      "INFO:tensorflow:loss = 6621.124, step = 5601 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7179\n",
      "INFO:tensorflow:loss = 6528.835, step = 5701 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.1839\n",
      "INFO:tensorflow:loss = 6555.9736, step = 5801 (1.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8531\n",
      "INFO:tensorflow:loss = 6469.6963, step = 5901 (1.237 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.2179\n",
      "INFO:tensorflow:loss = 6540.3086, step = 6001 (1.231 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4825\n",
      "INFO:tensorflow:loss = 6493.3223, step = 6101 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.687\n",
      "INFO:tensorflow:loss = 6503.1304, step = 6201 (1.240 sec)\n",
      "INFO:tensorflow:global_step/sec: 84.8873\n",
      "INFO:tensorflow:loss = 6514.4697, step = 6301 (1.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.009\n",
      "INFO:tensorflow:loss = 6518.649, step = 6401 (1.219 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7945\n",
      "INFO:tensorflow:loss = 6516.8594, step = 6501 (1.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.4459\n",
      "INFO:tensorflow:loss = 6466.4355, step = 6601 (1.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6863\n",
      "INFO:tensorflow:loss = 6433.21, step = 6701 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1139\n",
      "INFO:tensorflow:loss = 6478.2065, step = 6801 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1729\n",
      "INFO:tensorflow:loss = 6465.7124, step = 6901 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7671\n",
      "INFO:tensorflow:loss = 6484.8716, step = 7001 (1.253 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6971\n",
      "INFO:tensorflow:loss = 6492.8047, step = 7101 (1.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7639\n",
      "INFO:tensorflow:loss = 6451.8774, step = 7201 (1.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6852\n",
      "INFO:tensorflow:loss = 6491.828, step = 7301 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1856\n",
      "INFO:tensorflow:loss = 6482.924, step = 7401 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1402\n",
      "INFO:tensorflow:loss = 6563.9224, step = 7501 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.103\n",
      "INFO:tensorflow:loss = 6495.793, step = 7601 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0379\n",
      "INFO:tensorflow:loss = 6534.299, step = 7701 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.4858\n",
      "INFO:tensorflow:loss = 6471.748, step = 7801 (1.258 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.862\n",
      "INFO:tensorflow:loss = 6478.8325, step = 7901 (1.267 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9899\n",
      "INFO:tensorflow:loss = 6470.158, step = 8001 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1381\n",
      "INFO:tensorflow:loss = 6403.887, step = 8101 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.2531\n",
      "INFO:tensorflow:loss = 6416.6704, step = 8201 (1.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8092\n",
      "INFO:tensorflow:loss = 6440.299, step = 8301 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.8383\n",
      "INFO:tensorflow:loss = 6401.4443, step = 8401 (1.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8537\n",
      "INFO:tensorflow:loss = 6445.3115, step = 8501 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4613\n",
      "INFO:tensorflow:loss = 6419.4307, step = 8601 (1.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.8323\n",
      "INFO:tensorflow:loss = 6446.888, step = 8701 (1.236 sec)\n",
      "INFO:tensorflow:global_step/sec: 72.5098\n",
      "INFO:tensorflow:loss = 6388.2373, step = 8801 (1.379 sec)\n",
      "INFO:tensorflow:global_step/sec: 77.3417\n",
      "INFO:tensorflow:loss = 6465.589, step = 8901 (1.293 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0594\n",
      "INFO:tensorflow:loss = 6447.1084, step = 9001 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.9343\n",
      "INFO:tensorflow:loss = 6476.003, step = 9101 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.9737\n",
      "INFO:tensorflow:loss = 6380.706, step = 9201 (1.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8488\n",
      "INFO:tensorflow:loss = 6484.3896, step = 9301 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.505\n",
      "INFO:tensorflow:loss = 6407.294, step = 9401 (1.227 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1616\n",
      "INFO:tensorflow:loss = 6407.5127, step = 9501 (1.217 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6833\n",
      "INFO:tensorflow:loss = 6431.4526, step = 9601 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1126\n",
      "INFO:tensorflow:loss = 6389.8477, step = 9701 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0835\n",
      "INFO:tensorflow:loss = 6451.8335, step = 9801 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7777\n",
      "INFO:tensorflow:loss = 6399.582, step = 9901 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.8872\n",
      "INFO:tensorflow:loss = 6439.5674, step = 10001 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6849\n",
      "INFO:tensorflow:loss = 6396.6377, step = 10101 (1.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.1868\n",
      "INFO:tensorflow:loss = 6411.2744, step = 10201 (1.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.7916\n",
      "INFO:tensorflow:loss = 6399.2734, step = 10301 (1.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 79.8969\n",
      "INFO:tensorflow:loss = 6400.419, step = 10401 (1.251 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6083\n",
      "INFO:tensorflow:loss = 6387.8, step = 10501 (1.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.6707\n",
      "INFO:tensorflow:loss = 6346.918, step = 10601 (1.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7615\n",
      "INFO:tensorflow:loss = 6403.6436, step = 10701 (1.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0354\n",
      "INFO:tensorflow:loss = 6371.925, step = 10801 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4558\n",
      "INFO:tensorflow:loss = 6357.701, step = 10901 (1.243 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4606\n",
      "INFO:tensorflow:loss = 6422.9824, step = 11001 (1.228 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0908\n",
      "INFO:tensorflow:loss = 6346.512, step = 11101 (1.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 78.5748\n",
      "INFO:tensorflow:loss = 6361.6426, step = 11201 (1.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.7149\n",
      "INFO:tensorflow:loss = 6349.4995, step = 11301 (1.239 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.4461\n",
      "INFO:tensorflow:loss = 6420.7085, step = 11401 (1.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.7217\n",
      "INFO:tensorflow:loss = 6387.3447, step = 11501 (1.222 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.0389\n",
      "INFO:tensorflow:loss = 6370.4316, step = 11601 (1.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.9918\n",
      "INFO:tensorflow:loss = 6413.4897, step = 11701 (1.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6653\n",
      "INFO:tensorflow:loss = 6326.582, step = 11801 (1.225 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.173\n",
      "INFO:tensorflow:loss = 6359.6016, step = 11901 (1.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1661\n",
      "INFO:tensorflow:loss = 693.546, step = 12001 (1.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0461\n",
      "INFO:tensorflow:loss = 6062.6113, step = 12101 (1.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.935\n",
      "INFO:tensorflow:loss = 6013.257, step = 12201 (1.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.885\n",
      "INFO:tensorflow:loss = 5935.288, step = 12301 (1.221 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.1122\n",
      "INFO:tensorflow:loss = 6067.889, step = 12401 (1.218 sec)\n",
      "INFO:tensorflow:global_step/sec: 82.7924\n",
      "INFO:tensorflow:loss = 6052.6226, step = 12501 (1.208 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.0793\n",
      "INFO:tensorflow:loss = 6042.1445, step = 12601 (1.203 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.1769\n",
      "INFO:tensorflow:loss = 6088.8477, step = 12701 (1.248 sec)\n",
      "INFO:tensorflow:global_step/sec: 80.4063\n",
      "INFO:tensorflow:loss = 6135.7817, step = 12801 (1.245 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6928\n",
      "INFO:tensorflow:loss = 6156.3496, step = 12901 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.0626\n",
      "INFO:tensorflow:loss = 6174.788, step = 13001 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.1825\n",
      "INFO:tensorflow:loss = 6186.65, step = 13101 (1.233 sec)\n",
      "INFO:tensorflow:global_step/sec: 81.6319\n",
      "INFO:tensorflow:loss = 6225.307, step = 13201 (1.224 sec)\n",
      "INFO:tensorflow:global_step/sec: 83.8057\n",
      "INFO:tensorflow:loss = 6248.757, step = 13301 (1.193 sec)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10292/756738304.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mretrieval_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mretrieval_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1193\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m   1194\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m                                              saving_listeners)\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[0;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[1;32m   1492\u001b[0m       \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1494\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1495\u001b[0m         \u001b[0many_step_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many_step_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1259\u001b[0;31m             run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1260\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m         logging.info(\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1345\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "retrieval_model = tf.estimator.Estimator(model_fn=model_fn)\n",
    "retrieval_model.train(input_fn=input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poeem_model_fn(features, labels, mode, params):\n",
    "    query_column = tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key='query',\n",
    "            vocabulary_list=range(VOCAB_SIZE),\n",
    "            dtype=tf.int32),\n",
    "        dimension=EMB_DIM)\n",
    "    query_emb = tf.feature_column.input_layer(features, [query_column])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        ### [poeem code] directly do ANN search in the model as a TensorFlow op.\n",
    "        if params.get('item_search', False):\n",
    "            index = poeem.search.index_from_file(params['index_file'])\n",
    "            neighbors, scores = index.search(\n",
    "                tf.expand_dims(query_emb, 1),\n",
    "                params['topk'],\n",
    "                params['nprobe'],\n",
    "                params['metric_type'],\n",
    "                verbose=False)        \n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, predictions={'neighbors': neighbors, 'scores': scores})\n",
    "        ### end [poeem code]\n",
    "        \n",
    "    item_column = tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "            key='item',\n",
    "            vocabulary_list=range(VOCAB_SIZE),\n",
    "            dtype=tf.int32),\n",
    "        dimension=EMB_DIM)\n",
    "    item_emb = tf.feature_column.input_layer(features, [item_column])\n",
    "    item_emb = tf.nn.l2_normalize(item_emb, axis=1)\n",
    "    \n",
    "\n",
    "    ### [poeem code] item indexing layer as the last layer in item tower\n",
    "    hparams = poeem.embedding.PoeemHparam(coarse_K=8,\n",
    "                        K=256,\n",
    "                        D=8,\n",
    "                        rotate=0) # exactly the same parameters as Faiss, specified above.\n",
    "    item_batch_quantized = poeem.embedding.PoeemEmbed(\n",
    "        EMB_DIM,\n",
    "        warmup_steps=16384,\n",
    "        buffer_size=8192,\n",
    "        hparams=hparams,\n",
    "        mode=mode)\n",
    "    \n",
    "    # gradient straight-through estimator. For details, check out our paper.\n",
    "    item_emb_tau, coarse_code, code, regularizer = item_batch_quantized.forward(item_emb)\n",
    "    item_emb = item_emb - tf.stop_gradient(item_emb - item_emb_tau)\n",
    "    ### end [poeem code] \n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        ### [poeem code] exprt item embeddings/PQ code for disk persistency.\n",
    "        if params.get('item_predict', False):\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, predictions={\n",
    "                    'item_coarse_code': coarse_code,\n",
    "                    'item_code': code,\n",
    "                    'item_norm': tf.norm(item_emb, axis=1)\n",
    "                })\n",
    "        ### end [poeem code] \n",
    "\n",
    "    def cosine(a, b):\n",
    "        a = tf.nn.l2_normalize(a, axis=1)\n",
    "        b = tf.nn.l2_normalize(b, axis=1)\n",
    "        return tf.matmul(a, b, transpose_b=True)\n",
    "\n",
    "    scores = cosine(query_emb, item_emb)\n",
    "\n",
    "    batch_size = tf.shape(query_emb)[0]\n",
    "    loss = tf.reduce_sum(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=tf.eye(batch_size),\n",
    "            logits=scores * 30))\n",
    "    \n",
    "    loss = loss + regularizer\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(LEARNING_RATE)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, train_op=train_op, predictions={'query': query_emb, 'item': item_emb})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "511b5ca9ee75d4bd839c5bb363ab14e371609509931906521ab58ebd06a2f4bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
